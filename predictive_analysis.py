# -*- coding: utf-8 -*-
"""Predictive Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UyorH_ZmJPvbPn0kuwWtCmJgnmGsnsZS
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from imblearn.over_sampling import SMOTE
import pandas as pd
import numpy as np
import uvicorn
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize the FastAPI app
app = FastAPI()

# Global variables
scaler = None
model = None
dataset = None

@app.get("/")
async def read_root():
    return {"message": "Welcome to the Predictive Analysis API!"}

@app.post("/upload/")
async def upload_file(file: UploadFile = File(...)):
    global dataset
    try:
        contents = await file.read()
        with open("uploaded_dataset.csv", "wb") as f:
            f.write(contents)
        dataset = pd.read_csv("uploaded_dataset.csv")
        return {
            "message": f"File '{file.filename}' uploaded successfully",
            "columns": dataset.columns.tolist()
        }
    except Exception as e:
        logger.error(f"Error processing upload: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/train")
async def train_model():
    global dataset, model, scaler
    try:
        if dataset is None:
            raise HTTPException(status_code=400, detail="No dataset uploaded. Please upload a dataset first.")

        X = dataset[["Temperature", "Run_Time"]]
        y = dataset["Downtime_Flag"]

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        smote = SMOTE(random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

        model = LogisticRegression(C=1.0, penalty='l2', solver='lbfgs', max_iter=1000, random_state=42)
        model.fit(X_train_resampled, y_train_resampled)

        y_pred = model.predict(X_test_scaled)

        metrics = {
            "accuracy": np.round(accuracy_score(y_test, y_pred), 3),
            "f1_score": np.round(f1_score(y_test, y_pred), 3),
            "precision": np.round(precision_score(y_test, y_pred), 3),
            "recall": np.round(recall_score(y_test, y_pred), 3)
        }

        return {"message": "Model trained successfully", **metrics}

    except Exception as e:
        logger.error(f"Error training model: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/predict")
async def predict_downtime(data: dict):
    global model, scaler
    try:
        if model is None or scaler is None:
            raise HTTPException(status_code=400, detail="Model not trained. Please train the model first.")

        required_fields = ["Temperature", "Run_Time"]
        if not all(field in data for field in required_fields):
            raise HTTPException(
                status_code=400,
                detail=f"Missing required fields. Please provide: {required_fields}"
            )

        input_data = pd.DataFrame([data])[required_fields]
        input_scaled = scaler.transform(input_data)

        prediction = model.predict(input_scaled)[0]
        probability = model.predict_proba(input_scaled)[0][1]

        return {
            "Downtime": "Yes" if prediction == 1 else "No",
            "Confidence": round(float(probability), 3)
        }

    except Exception as e:
        logger.error(f"Prediction error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="127.0.0.1", port=8000)





